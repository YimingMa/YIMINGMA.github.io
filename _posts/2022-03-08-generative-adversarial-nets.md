---
key: 2022_03_08_01
title: Introduction to Generative Adversarial Networks
tags: ["Computer Vision", "Self-Supervised Learning"]
mathjax: true
mathjax_autoNumber: true
author: Yiming
comment: false
pageview: false
aside:
    toc: true
---

**Claim**: This post refers to many materials of [Understanding Generative Adversarial Networks (GANs)](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29) by Joseph Rocca and Baptiste Rocca. The original paper in which GANs are proposed is called *Generative Adversarial Nets* by Ian Goodfellow et al.

## Introduction

Yann LeCun described it as “the most interesting idea in the last 10 years in Machine Learning”. Of course, such a compliment coming from such a prominent researcher in the deep learning area is always a great advertisement for the subject we are talking about! And, indeed, **Generative Adversarial Networks** (**GANs** for short) have had a huge success since they were introduced in 2014 by Ian J. Goodfellow and co-authors in the article [Generative Adversarial Nets](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf).

Before going into the details, let’s give a quick overview of what GANs are made for. Generative adversarial networks belong to the set of **generative** models, which are able to generate new content. To illustrate this notion of “generative models”, we can take a look at some well known examples of results obtained with GANs.

<figure>
  <img src="/posts.assets/2022-03-08-generative-adversarial-nets.assets/visualisations.png" alt="visualisations" style="width:100%">
  <figcaption>Illustration of GANs abilities by Ian Goodfellow and co-authors. These images are samples generated by GANs after training on two datasets: MNIST and TFD. For both, the rightmost column contains real images in the training set that are the nearest from the direct neighbouring generated samples. The comparison between them show that the produced data are really generated and not only memorised by the network.</figcaption>
</figure>

## Sampling from Distributions

In this section, we discuss the process of generating random numbers: we review some existing methods and more especially the inverse transform method that draws samples from complicated distributions based on the uniform distribution. Although all this could seems a little bit far from our subject of matter, GANs, we will see in the next section the deep link that exists with generative models.

### Uniform Random Numbers

Computers are fundamentally deterministic, so theoretically, it is impossible to generate numbers that are really random. However, numbers generated by certain algorithms do share similar properties of theoretical random numbers. <span style="color:Coral;">In particular, a computer is able to generate a sequence of numbers that approximatively follows the uniform distribution between 0 and 1, by using a pseudorandom number generator</span>. The uniform case is a very simple one upon which more complex distributions can be sampled in different ways.

