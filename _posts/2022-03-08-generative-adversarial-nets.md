---
key: 2022_03_08_01
title: Introduction to Generative Adversarial Networks
tags: ["Computer Vision", "Self-Supervised Learning"]
mathjax: true
mathjax_autoNumber: true
author: Yiming
comment: false
pageview: false
aside:
    toc: true
---

**Claim**: This post refers to many materials of [Understanding Generative Adversarial Networks (GANs)](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29) by Joseph Rocca and Baptiste Rocca. The original paper in which GANs are proposed is called *Generative Adversarial Nets* by Ian Goodfellow et al.

## Introduction

Yann LeCun described it as "the most interesting idea in the last 10 years in Machine Learning". Of course, such a compliment coming from such a prominent researcher in the deep learning area is always a great advertisement for the subject we are talking about! And, indeed, **Generative Adversarial Networks** (**GANs** for short) have had a huge success since they were introduced in 2014 by Ian J. Goodfellow and co-authors in the article [Generative Adversarial Nets](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf).

Before going into the details, let's give a quick overview of what GANs are made for. Generative adversarial networks belong to the set of **generative** models, which are able to generate new content. To illustrate this notion of "generative models", we can take a look at some well known examples of results obtained with GANs.

<figure>
  <img src="/posts.assets/2022-03-08-generative-adversarial-nets.assets/visualisations.png" alt="visualisations" style="width:100%">
  <figcaption>Illustration of GANs abilities by Ian Goodfellow and co-authors. These images are samples generated by GANs after training on two datasets: MNIST and TFD. For both, the rightmost column contains real images in the training set that are the nearest from the direct neighbouring generated samples. The comparison between them show that the produced data are really generated and not only memorised by the network.</figcaption>
</figure>

## Sampling from Distributions

In this section, we discuss the process of generating random numbers: we review some existing methods and more especially the inverse transform method that draws samples from complicated distributions based on the uniform distribution. Although all this could seems a little bit far from our subject of matter, GANs, we will see in the next section the deep link that exists with generative models.

### Uniform Random Numbers

Computers are fundamentally deterministic, so theoretically, it is impossible to generate numbers that are really random. However, numbers generated by certain algorithms do share similar properties of theoretical random numbers. <span style="color:Coral;">In particular, a computer is able to generate a sequence of numbers that approximatively follows the uniform distribution between 0 and 1, by using a pseudorandom number generator</span>. The uniform case is a very simple one upon which more complex distributions can be sampled in different ways.

### The Inverse Transform Method

Let $$X$$ be a random variable with the cumulative distribution function $$F_X( \cdot )$$, and we would like to draw samples from this distribution. Let $$U$$ be a uniform random variable over $$[0, \; 1]$$, and we know its CDF can be expressed as

$$F_U(x) = \mathbb{P}(U \le x) = x. \notag$$

For simplicity, we suppose that the function $$F_X$$ is invertible and its inverse is denoted as $$F_X^{-1}$$. (If $$F_X$$ is not invertible, then we can modify the pseudo-inverse’s values on a zero-measure subset to make it a proper function.) Then we have $$Y := F_X(X) \sim U(0, \; 1)$$, because

$$
\begin{align*}
F_Y(x) & = \mathbb{P}\left( Y \le x\right) \\
& = \mathbb{P}\left( F_X(X) \le x \right) \\
& = \mathbb{P}\left( X \le F_X^{-1}(x) \right) \quad (\text{since $F_X$ is invertible)} \\
& = F_X \left( F_X^{-1} (x) \right) \\
& = x
\end{align*}
$$

Thus, to sample from the distribution of $$X$$, we can first draw samples $$\{ u_1, \, u_2, \, \cdots, u_n\}$$ from $$U(0, \; 1)$$, then transform them with the inverse $$F_X^{-1}$$:

$$
\left\{ F_X^{-1} (u_1), \, F_X^{-1} (u_2), \, \cdots, F_X^{-1} (u_n) \right\}. \notag
$$

To summarise, the inverse transform method is a way to generate samples of a distribution that leverages uniform random numbers going through a well designed "transform function" (the inverse CDF). This notion can be extended to a more generalised idea, in which complicated random variables can be expressed as some simpler random variables (not necessarily uniform and then the transform functions are no longer the inverse CDF). Conceptually, the purpose of the "transform function" is to deform/reshape the initial probability distribution: the transform function takes from where the initial distribution is too high compared to the targeted distribution and puts it where it is too low.

<figure>
  <img src="/posts.assets/2022-03-08-generative-adversarial-nets.assets/inverse_transform_method.png" alt="visualisations" style="width:100%">
  <figcaption>Illustration of the inverse transform method. In blue: the uniform distribution over [0,1]. In orange: the the standard Gaussian distribution. In grey: the mapping from the uniform to the gaussian distribution (inverse CDF).</figcaption>
</figure>

## Generative Models

### Problem Formulation

Suppose that we are interested in generating black and white square images of dogs with the size of $$n$$ by $$n$$ pixels. We can reshape each image as an $$N = n \times n$$ dimensional vector.  However, it doesn’t mean that all $$N$$-dimensional vectors represent dogs once shaped back to a square! So, we can say that only some $$N$$-dimensional vectors are associated with images of dogs, and they follow certain distribution over the $$N$$-dimensional vector space. In the same spirit, there exist, over this $$N$$-dimensional vector space, probability distributions for images of cats, birds and so on.

<p style="color:Crimson;">Then, the problem of generating a new image of dog is equivalent to the problem of generating a new vector following the "dog probability distribution" over the $N$-dimensional vector space. So we are, in fact, facing a problem of sampling a random variable with respect to a specific probability distribution.</p>

At this point, we can mention two important issues. First the "dog probability distribution" we mentioned is a very complex distribution over a very large space. Second, even if we can assume the existence of such underlying distribution (there actually exist images that look like dogs and others that don’t), we obviously don’t know how to express it explicitly. These two problems mean that we can’t directly utilise classic sampling approaches, such as the inverse transform method.

### Fit the "Inverse Transform Function"

Suppose the random vector of dogs' images, denoted by $$\boldsymbol{X} \in \mathbb{R}^N$$, can be expressed as a function of an $$N$$-dimensional uniform random vector. Thus, to generate a new dog image, we only need to draw a sample from the $$N$$-dimensional uniform distribution and feed it to the function to acquire an observation of $$\boldsymbol{X}$$. Finally, we reshape the vector into a desired $$n \times n$$ image. However, due to the extremely complicated form of the transform function (the close form may even not exist), we have to use neural networks to fit the inverse.

<figure>
  <img src="/posts.assets/2022-03-08-generative-adversarial-nets.assets/generative_models.png" alt="visualisations" style="width:100%">
  <figcaption>Illustration of the notion of generative models using neural networks. Obviously, the dimensionality we are really talking about are much higher than represented here.</figcaption>
</figure>

## Generative Matching Networks

**Disclaimer**: The denomination of "Generative Matching Networks" is not a standard one. However, we can find in the literature, for example, "Generative Moments Matching Networks" or also "Generative Features Matching Networks". We just want here to use a slightly more general denomination for what we describe bellow.

### Training Generative models

So far, we have shown that our problem of generating a new image of a dog can be rephrased into a problem of generating a random vector in the $$N$$-dimensional vector space that follows the "dog probability distribution". We have also suggested that the random vector can be generated via the transform method, with a neural network modelling the transform function.

Now, we still need to train (optimise) the network to express the right transform function. To this purpose, we can suggest two different training methods: a direct one and an indirect one.

- <span style="color:RoyalBlue;">In the direct training method, there are predefined metrics estimating the distance between the ground-truth distribution and the generated one, so that the error can be directly back-propagated through the network. This is the idea that rules Generative Matching Networks (GMNs).<span>
- <span style="color:RoyalBlue;">In the indirect training method, we do not defined metrics for distribution distances. Instead, we also train a downstream network which differentiates samples from the ground-truth distribution and those from the generated distribution. The optimisation process of the overall network with respect to the downstream task will enforce the generated distribution to be close to the true distribution. This idea is the one behind Generative Adversarial Networks (GANs).<span>

But for now, let's start with the direct method and GMNs.

### Distance of Two Probability Distributions Based on Samples

As mentioned, the idea of GMNs is to train the generative network by directly comparing the generated distribution with the true one. However, we do not know how to express them explicitly, so comparisons based on explicit expressions are not possible. On the other hand,  we can estimate the distance between distributions based on samples. Indeed, we have a sample of real dog images, and we can produce a sample of generated data at each iteration of the training process.

In theory, any distance (or similarity measure) able to compare effectively two distributions based on samples can be used, and here we exploit the **Maximum Mean Discrepancy** (**MMD**) approach. Although it is not fully out of the scope of this article, we have decided not to spend much more time describing the MMD. The readers that would like to know more about MMD right now can refer to [these slides](http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf), [this article](http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf) or [this article](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf).

### Back Propagation of MMD

So, once we have defined a metric to estimate the distance between two distributions based on samples, we can design the training process of the generative network in GMNs. Given samples from the $$N$$-dimensional uniform distribution as input, we would like the probability distribution of the generated output to follow the "dog probability distribution". The idea of GMNs is then to optimise the network by repeating the following steps:

1. Generate some $$N$$-dimensional vectors that follow the uniform distribution.
2. Feed these vectors to the network and collect the generated output vectors, which are the generated dog vectors.
3. Estimate the distance between distributions based on the real dog data with the generated ones by some metrics (e.g., MMD).
4. Use back propagation to make one step of gradient descent to lower the distance (e.g., MMD) between true and generated distributions

As written above, when following these steps we are applying a gradient descent over the network with a loss function that is the distance between the true and the generated distributions at the current iteration.

<figure>
  <img src="/posts.assets/2022-03-08-generative-adversarial-nets.assets/GMN.png" alt="visualisations" style="width:100%">
  <figcaption>Generative matching networks take simple random inputs, generate new data, directly compare the distribution of the generated data to the distribution of the true data and backpropagate the matching error to train the network. In short, it tries to fit the transform function.</figcaption>
</figure>

## Generative Adversarial Networks

### The "Indirect" Training Method

During training, GMNs compare the distribution of generated samples with that of the ground-truth samples directly, since we are using some pre-defined methods to estimate the distance between distributions based on samples. GANs instead replace these sample-based metrics by a downstream task. Since our goal is to generate a dog image that looks like real images, the downstream task is to discriminate between the real ones and synthesised ones. Or, we could say a “non-discrimination” task as we want the discrimination to fail as much as possible. <span style="color:RoyalBlue;">So, in a GAN architecture, we have a **discriminator**, that takes samples of true and generated data and that try to classify them as well as possible, and a **generator** that is trained to fool the discriminator as much as possible.</span> Let’s see on a simple example why the direct and indirect approaches we mentioned should, in theory, lead to the same optimal generator.

### The Ideal Case: Perfect Generator and Discriminator

In order to better understand why training a generator to fool a discriminator will lead to the same result as training directly the generator to match the target distribution, let's take a simple one dimensional example. We forget, for the moment, how both generator and discriminator are represented and consider them as abstract notions. Moreover, both are supposed to be "perfect" (with infinite capacities) in the sense that they are not constrained by any kind of (parametrised) model.

Suppose that we have a ground-truth distribution, for example a 1-D Gaussian and that we want a generator that samples from this probability distribution. In the "direct" method GMNs, the generator is iteratively adjusted to correct the measured distance between the true and the generated distributions. If we assume that the optimisation process is perfect, then theoretically, the generated distribution in the end should exactly match the true one.

<figure>
  <img src="/posts.assets/2022-03-08-generative-adversarial-nets.assets/direct_matching_method.png" alt="Direct Matching Method" style="width:100%">
  <figcaption>Illustration of the concept of "direct" matching method. The distribution in blue is the true one while the generated distribution is depicted in orange. Iteration by iteration, we compare the two distributions and adjust the networks weights through gradient descent steps. Here the comparison is done over the mean and the variance (similar to a truncated moments matching method). Notice that (obviously) this example is so simple that it doesn’t require an iterative approach: the purpose is only to illustrate the intuition given above.</figcaption>
</figure>