---
key: 2022_03_08_01
title: Introduction to Generative Adversarial Networks
tags: ["Computer Vision", "Self-Supervised Learning"]
mathjax: true
mathjax_autoNumber: true
author: Yiming
comment: false
pageview: false
aside:
    toc: true
---

**Claim**: This post refers to many materials of [Understanding Generative Adversarial Networks (GANs)](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29) by Joseph Rocca and Baptiste Rocca. The original paper in which GANs are proposed is called *Generative Adversarial Nets* by Ian Goodfellow et al.

## Introduction

Yann LeCun described it as “the most interesting idea in the last 10 years in Machine Learning”. Of course, such a compliment coming from such a prominent researcher in the deep learning area is always a great advertisement for the subject we are talking about! And, indeed, **Generative Adversarial Networks** (**GANs** for short) have had a huge success since they were introduced in 2014 by Ian J. Goodfellow and co-authors in the article [Generative Adversarial Nets](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf).

Before going into the details, let’s give a quick overview of what GANs are made for. Generative adversarial networks belong to the set of **generative** models, which are able to generate new content. To illustrate this notion of “generative models”, we can take a look at some well known examples of results obtained with GANs.

<figure>
  <img src="/posts.assets/2022-03-08-generative-adversarial-nets.assets/visualisations.png" alt="visualisations" style="width:100%">
  <figcaption>Illustration of GANs abilities by Ian Goodfellow and co-authors. These images are samples generated by GANs after training on two datasets: MNIST and TFD. For both, the rightmost column contains real images in the training set that are the nearest from the direct neighbouring generated samples. The comparison between them show that the produced data are really generated and not only memorised by the network.</figcaption>
</figure>

## Sampling from Distributions

In this section, we discuss the process of generating random numbers: we review some existing methods and more especially the inverse transform method that draws samples from complicated distributions based on the uniform distribution. Although all this could seems a little bit far from our subject of matter, GANs, we will see in the next section the deep link that exists with generative models.

### Uniform Random Numbers

Computers are fundamentally deterministic, so theoretically, it is impossible to generate numbers that are really random. However, numbers generated by certain algorithms do share similar properties of theoretical random numbers. <span style="color:Coral;">In particular, a computer is able to generate a sequence of numbers that approximatively follows the uniform distribution between 0 and 1, by using a pseudorandom number generator</span>. The uniform case is a very simple one upon which more complex distributions can be sampled in different ways.

### The Inverse Transform Method

Let $$X$$ be a random variable with the cumulative distribution function $$F_X( \cdot )$$, and we would like to draw samples from this distribution. Let $$U$$ be a uniform random variable over $$[0, \; 1]$$, and we know its CDF can be expressed as

$$F_U(x) = \mathbb{P}(U \le x) = x.$$

For simplicity, we suppose that the function $$F_X$$ is invertible and its inverse is denoted as $$F_X^{-1}$$. (If $$F_X$$ is not invertible, then we can modify the pseudo-inverse’s values on a zero-measure subset to make it a proper function.) Then we have $$Y := F_X(X) \sim U(0, \; 1)$$, because

$$
\begin{align*}
F_Y(x) & = \mathbb{P}\left( Y \le x\right) \\
& = \mathbb{P}\left( F_X(X) \le x \right) \\
& = \mathbb{P}\left( X \le F_X^{-1}(x) \right) \quad (\text{since $F_X$ is invertible)} \\
& = F_X \left( F_X^{-1} (x) \right) \\
& = x
\end{align*}
$$

Thus, to sample from the distribution of $$X$$, we can first draw samples $$\{ u_1, \, u_2, \, \cdots, u_n\}$$ from $$U(0, \; 1)$$, then transform them with the inverse $$F_X^{-1}$$:

$$
\left\{ F_X^{-1} (u_1), \, F_X^{-1} (u_2), \, \cdots, F_X^{-1} (u_n) \right\}.
$$

To summarise, the inverse transform method is a way to generate samples of a distribution that leverages uniform random numbers going through a well designed “transform function” (the inverse CDF). This notion can be extended to a more generalised idea, in which complicated random variables can be expressed as some simpler random variables (not necessarily uniform and then the transform functions are no longer the inverse CDF). Conceptually, the purpose of the “transform function” is to deform/reshape the initial probability distribution: the transform function takes from where the initial distribution is too high compared to the targeted distribution and puts it where it is too low.

<figure>
  <img src="/posts.assets/2022-03-08-generative-adversarial-nets.assets/inverse_transform_method.png" alt="visualisations" style="width:100%">
  <figcaption>Illustration of the inverse transform method. In blue: the uniform distribution over [0,1]. In orange: the the standard Gaussian distribution. In grey: the mapping from the uniform to the gaussian distribution (inverse CDF).</figcaption>
</figure>

## Generative Models

### Problem Formulation

Suppose that we are interested in generating black and white square images of dogs with the size of $$n$$ by $$n$$ pixels. We can reshape each image as an $$N = n \times n$$ dimensional vector.  However, it doesn’t mean that all $$N$$-dimensional vectors represent dogs once shaped back to a square! So, we can say that only some $$N$$-dimensional vectors are associated with images of dogs, and they follow certain distribution over the $$N$$-dimensional vector space. In the same spirit, there exist, over this $$N$$-dimensional vector space, probability distributions for images of cats, birds and so on.

<p style="color:Crimson;">Then, the problem of generating a new image of dog is equivalent to the problem of generating a new vector following the “dog probability distribution” over the $N$-dimensional vector space. So we are, in fact, facing a problem of sampling a random variable with respect to a specific probability distribution.</p>

At this point, we can mention two important issues. First the “dog probability distribution” we mentioned is a very complex distribution over a very large space. Second, even if we can assume the existence of such underlying distribution (there actually exist images that look like dogs and others that don’t), we obviously don’t know how to express it explicitly. These two problems mean that we can’t directly utilise classic sampling approaches, such as the inverse transform method.
